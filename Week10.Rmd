---
title: "Untitled"
author: '500033689'
date: "2023-05-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Loading in packages}
library(tidyverse)
library(readxl)
library(janitor)
library(sf)
```

```{r Loading in the data}
catch_95 <- read.csv("CatchInd1995_1999.csv") %>% clean_names()
catch_00 <- read.csv("CatchInd2000_2004.csv") %>% clean_names()
catch_05 <- read.csv("CatchInd2005_2009.csv") %>% clean_names()
catch_10 <- read.csv("CatchInd2010_2014.csv") %>% clean_names()

cell <- read_xlsx("Codes.xlsx", sheet = "Cell") %>% clean_names()
gear <- read_xlsx("Codes.xlsx", sheet = "Gear") %>% clean_names()
taxa <- read_xlsx("Codes.xlsx", sheet = "Taxa") %>% clean_names()
country <- read_xlsx("Codes.xlsx", sheet = "Country") %>% clean_names()

```

```{r}
index <- read.csv("IndexInd.csv") %>% clean_names()
```


Each ID corresponds to a taxa-country-gear-year combination.
For example, pelagic fishes caught by Chinese fishing vessels using squid hooks in 1970.
For this report, we focus on yellowfin tuna caught by Indonesian fishing vessels using any type of fishing gear.
We start by filtering only for Indonesian yellowfin tuna records, and combining across the different year ranges.

```{r Filtering for Indonesian records}
indo_code <- country %>% filter(fao_name == "Indonesia") %>% pull(country)
tuna_code <- taxa %>% filter(common_name == "Yellowfin tuna") %>% pull(taxon_key)
indo_tuna_ids <- index %>% filter(c_number == indo_code, taxonkey == tuna_code) %>% pull(id)

catch_95_filtered <- catch_95 %>% filter(id %in% indo_tuna_ids)
catch_00_filtered <- catch_00 %>% filter(id %in% indo_tuna_ids)
catch_05_filtered <- catch_05 %>% filter(id %in% indo_tuna_ids)
catch_10_filtered <- catch_10 %>% filter(id %in% indo_tuna_ids)

catch_all <- rbind(catch_95_filtered, catch_00_filtered, catch_05_filtered, catch_10_filtered)
```

We now:

1. Merge in the location, year, and gear data.
2. Filter down to the Indonesian region (we were only considering Indonesian vessels before).
3. Aggregate results across fishing gears.

```{r Merging the data}
catch_cleaned <- catch_all %>% 
  left_join(cell, by = "cell") %>% 
  left_join(index[, c("id", "i_year", "gear")], by = "id") %>% 
  filter(lat_centre >= -10, lat_centre <= 13, lon_centre >= 93, lon_centre <= 134) %>% 
  group_by(lat_centre, lon_centre, i_year) %>% 
  summarise(sum_reported = sum(reported),
            area = mean(ocean_areasqkm)) %>% 
  ungroup() %>% 
  arrange(lat_centre, lon_centre, i_year) %>% 
  rename(latitude = lat_centre, longitude = lon_centre, year = i_year) %>% 
  mutate(rate_reported = round(1000*sum_reported/area, 4)) %>% # Landings per 1000 square kilometres
  select(-sum_reported, -area)
```

We now load in the reef data from the labs and filter it to have the same range as the fishing data.

```{r Loading in the reef data}
reef <- read.csv("Reef_Check_with_cortad_variables_with_annual_rate_of_SST_change.csv") %>% 
  clean_names() %>% 
  filter(latitude_degrees >= min(catch_cleaned$latitude), latitude_degrees <= max(catch_cleaned$latitude), 
         longitude_degrees >= min(catch_cleaned$longitude), longitude_degrees <= max(catch_cleaned$longitude),
         year >= min(catch_cleaned$year), year <= max(catch_cleaned$year)) %>% 
  select(latitude_degrees, longitude_degrees, year, average_bleaching, clim_sst) %>% 
  rename(latitude = latitude_degrees, longitude = longitude_degrees)
```

We perform a nearest-neighbour join between the reef and catch datasets.
To make things easier (?) we loop through and do processing for each year.

```{r Nearest-neighbour join}
merged <- data.frame()

for (curr_year in unique(reef$year)) {
  catch_sf <- catch_cleaned %>% 
    filter(year == curr_year) %>% 
    as.data.frame() %>% 
    st_as_sf(coords = c("longitude", "latitude"))
  
  reef_sf <- reef %>% 
    filter(year == curr_year) %>% 
    st_as_sf(coords = c("longitude", "latitude"))

  # Compute distances between each point in catch_sf and reef_sf
  distances <- st_distance(catch_sf, reef_sf)

  # Get the minimum distance for each catch point
  min_distances <- apply(distances, 1, min)

  # Join the datasets based on nearest features
  merged_year <- st_join(catch_sf, reef_sf, join = st_nearest_feature) %>% 
    as.data.frame() %>% 
    mutate(latitude = st_coordinates(.$geometry)[, 2], longitude = st_coordinates(.$geometry)[, 1]) %>% 
    select(-year.x, -year.y, -geometry) %>% 
    mutate(year = curr_year)

  # Add the minimum distance as a new column to the merged_year dataframe
  merged_year$distance_to_nearest_reef <- min_distances

  rownames(merged_year) <- NULL
  merged <- rbind(merged, merged_year)
}



```

```{r}
merged
```


We now load in and merge the SAU fishing effort data, before normalising the catch data by fishing effort.

```{r Fishing effort data}
effort <- read.csv("SAU Effort FishingEntity 83 v50-1.csv") %>% clean_names() %>% 
  filter(fishing_sector == "Industrial") %>% 
  group_by(year) %>% 
  summarise(sum_effort = sum(effort)) %>% 
  mutate(sum_effort = sum_effort/1e+06)

merged_effort <- merged %>% 
  left_join(effort, by = "year") %>% 
  filter(!is.na(sum_effort)) %>% 
  mutate(rate_norm = rate_reported/sum_effort) %>% 
  select(-rate_reported)

write.csv(merged_effort, file = "merged.csv")
```

We now perform some exploratory data analysis.

```{r EDA}
merged_effort %>% 
ggplot(aes(x = average_bleaching, y = rate_norm)) +
  geom_point()

merged_effort %>% 
  mutate(bleached = ifelse(average_bleaching > 0, "Bleached", "Not bleached")) %>% 
  ggplot(aes(x = bleached, y = log(rate_norm))) +
  geom_boxplot()



```
```{r}
library(caret)
set.seed(123)

# Split the data into a training and testing set
split_indices <- createDataPartition(merged_effort$average_bleaching, p = 0.8, list = FALSE)
training_set <- merged_effort[split_indices, ]
testing_set <- merged_effort[-split_indices, ]

```
```{r}
# Model 1: Temperature
model_temp <- lm(average_bleaching ~ clim_sst, data = training_set)

# Model 2: Fishing rate_norm
model_rate_norm <- lm(average_bleaching ~ rate_norm, data = training_set)

# Model 3: Fishing distance
model_distance <- lm(average_bleaching ~ distance_to_nearest_reef, data = training_set)


```

```{r}
# Model 4: Temperature + Fishing rate_norm + Fishing distance
model_all <- lm(average_bleaching ~ clim_sst + rate_norm + distance_to_nearest_reef, data = training_set)

```

```{r}
library(Metrics)

# Evaluate model performance
pred_temp <- predict(model_temp, testing_set)
pred_rate_norm <- predict(model_rate_norm, testing_set)
pred_distance <- predict(model_distance, testing_set)
pred_all <- predict(model_all, testing_set)

mse_temp <- mse(testing_set$average_bleaching, pred_temp)
mse_rate_norm <- mse(testing_set$average_bleaching, pred_rate_norm)
mse_distance <- mse(testing_set$average_bleaching, pred_distance)
mse_all <- mse(testing_set$average_bleaching, pred_all)

cat("MSE for Temperature model:", mse_temp, "\n")
cat("MSE for Fishing rate_norm model:", mse_rate_norm, "\n")
cat("MSE for Fishing distance model:", mse_distance, "\n")
cat("MSE for Combined model:", mse_all, "\n")

```
```{r}
library(ggplot2)

# Create a data frame for plotting
mse_data <- data.frame(
  Model = factor(c("Temperature", "Fishing rate_norm", "Fishing distance", "Combined")),
  MSE = c(mse_temp, mse_rate_norm, mse_distance, mse_all)
)

# Create the bar plot
ggplot(mse_data, aes(x = Model, y = MSE, fill = Model)) +
  geom_bar(stat = "identity", width = 0.7, show.legend = FALSE) +
  labs(title = "MSE of each model", x = "Model", y = "Mean Squared Error") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```
```{r}
# Install and load the randomForest package
library(randomForest)

# Create the random forest model
model_rf <- randomForest(average_bleaching ~ clim_sst + rate_norm + distance_to_nearest_reef, data = training_set, ntree = 500, mtry = 2, importance = TRUE)

# Evaluate the random forest model
pred_rf <- predict(model_rf, testing_set)
mse_rf <- mse(testing_set$average_bleaching, pred_rf)
cat("MSE for Random Forest model:", mse_rf, "\n")

# Plot the variable importance
varImpPlot(model_rf)

```
```{r}
# Create binary target variable
training_set$bleaching_occurred <- as.factor(ifelse(training_set$average_bleaching > 0, 1, 0))
testing_set$bleaching_occurred <- as.factor(ifelse(testing_set$average_bleaching > 0, 1, 0))


library(randomForest)

# Random Forest Model
rf_model <- randomForest(bleaching_occurred ~ clim_sst + rate_norm + distance_to_nearest_reef,
                         data = training_set)
rf_pred <- predict(rf_model, testing_set)

# Logistic Regression Model
logit_model <- glm(bleaching_occurred ~ clim_sst + rate_norm + distance_to_nearest_reef, 
                   data = training_set, family = binomial)
logit_pred <- predict(logit_model, testing_set, type = "response")
logit_pred <- ifelse(logit_pred > 0.5, 1, 0)

# Calculate accuracy
rf_accuracy <- mean(rf_pred == testing_set$bleaching_occurred)
logit_accuracy <- mean(logit_pred == testing_set$bleaching_occurred)

cat("Accuracy of Random Forest Model:", rf_accuracy, "\n")
cat("Accuracy of Logistic Regression Model:", logit_accuracy)

```

